1
00:00:01,740 --> 00:00:06,380
Speaker 0: You were a proponent of deep learning before it gained widespread acceptance.

2
00:00:07,601 --> 00:00:10,022
Speaker 0: What did you see in this field that gave you confidence?

3
00:00:10,402 --> 00:00:16,966
Speaker 0: What was your thinking process like in that first decade of the, I don't know what that's called, two thousands, the aughts?

4
00:00:17,966 --> 00:00:20,728
Speaker 1: Yeah, I can tell you the thing we got wrong and the thing we got right.

5
00:00:21,108 --> 00:00:26,991
Speaker 1: The thing we really got wrong was the importance of, the early importance of unsupervised learning.

6
00:00:27,871 --> 00:00:33,937
Speaker 1: So, early days of Google Brain, we put a lot of effort into unsupervised learning rather than supervised learning.

7
00:00:34,538 --> 00:00:48,571
Speaker 1: And there was this argument, I think it was around, um, two thousand five, after, you know, NeurIPS, at that time called NIPS, but now NeurIPS had ended, and Jeff Hinton and I were sitting in the cafeteria outside, you know, the conference, we had lunch, we were just chatting.

8
00:00:49,131 --> 00:00:52,735
Speaker 1: And Jeff pulled out this napkin, he started sketching this argument on the napkin.

9
00:00:52,915 --> 00:00:54,817
Speaker 1: It was very compelling, I'll repeat it.

10
00:00:56,058 --> 00:01:00,783
Speaker 1: Human brain has about a hundred trillion, so that's ten to the fourteen synaptic connections.

11
00:01:02,004 --> 00:01:04,807
Speaker 1: You will live for about ten to the nine seconds.

12
00:01:05,367 --> 00:01:06,168
Speaker 1: That's thirty years.

13
00:01:06,208 --> 00:01:09,972
Speaker 1: You actually live for two by ten to the nine, maybe three by ten to the nine seconds.

14
00:01:10,012 --> 00:01:11,173
Speaker 1: So just let's say ten to the nine.

15
00:01:12,234 --> 00:01:25,207
Speaker 1: So if each synaptic connection, each weight in your brain's neural network has just a one bit parameter, that's ten to the fourteen bits you need to learn in up to ten to the nine seconds of your life.

16
00:01:26,068 --> 00:01:33,295
Speaker 1: So via this simple argument, which is a lot of problems, it's very simplified, that's ten to the five bits per second you need to learn in your life.

17
00:01:34,055 --> 00:01:35,655
Speaker 1: And I have a one-year-old daughter.

18
00:01:35,676 --> 00:01:42,137
Speaker 1: I am not pointing out ten to five bits per second of labels to her.

19
00:01:43,177 --> 00:01:48,379
Speaker 1: So, and I think I'm a very loving parent, but I'm just not going to do that, right?

20
00:01:49,139 --> 00:01:56,961
Speaker 1: So from this, you know, very crude, definitely problematic argument, there's just no way that most of what we know is through supervised learning.

21
00:01:57,621 --> 00:02:02,402
Speaker 1: The way she gets so many bits of information is from sucking in images, audio, those experiences in the world.

22
00:02:03,803 --> 00:02:12,050
Speaker 1: And so that argument, and there are a lot of known flaws in this argument, you know, we should go into, really convinced me that there's a lot of power to unsupervised learning.

23
00:02:13,711 --> 00:02:16,613
Speaker 1: So that was the part that we actually maybe got wrong.

24
00:02:16,873 --> 00:02:24,960
Speaker 1: I still think unsupervised learning is really important, but in the early days, you know, ten, fifteen years ago, a lot of us thought that was the path forward.

25
00:02:25,500 --> 00:02:29,644
Speaker 0: Oh, so you're saying that that perhaps was the wrong intuition for the time.

26
00:02:29,844 --> 00:02:30,304
Speaker 1: For the time.

27
00:02:30,344 --> 00:02:31,786
Speaker 1: That was the part we got wrong.

28
00:02:32,802 --> 00:02:35,443
Speaker 1: The part we got right was the importance of scale.

29
00:02:35,843 --> 00:02:51,428
Speaker 1: So Adam Coates, another wonderful person, fortunate to have worked with him, he was in my group at Stanford at the time, and Adam had run these experiments at Stanford, showing that the bigger we train a learning algorithm, the better its performance.

30
00:02:52,188 --> 00:02:59,852
Speaker 1: And it was based on that, there was a graph that Adam generated, you know, where the x-axis, y-axis lines going up into the right.

31
00:02:59,892 --> 00:03:03,974
Speaker 1: So the bigger you make this thing, the better its performance, accuracy is the vertical axis.

32
00:03:04,555 --> 00:03:15,560
Speaker 1: So it's really based on that chart that Adam generated, that he gave me the conviction that we could scale these models way bigger than what we could on a few CPUs, which is what we had at Stanford, that we could get even better results.

33
00:03:15,780 --> 00:03:18,542
Speaker 1: And it was really based on that one figure that Adam generated.

34
00:03:19,242 --> 00:03:28,165
Speaker 1: that gave me the conviction to go with Sebastian Thrun to pitch, you know, starting a project at Google, which became the Google Brain Project.

35
00:03:28,185 --> 00:03:38,948
Speaker 0: Google Brain, you go find Google Brain, and there the intuition was scale will bring performance for the system, so we should chase a larger and larger scale.

36
00:03:39,648 --> 00:03:49,797
Speaker 0: And I think people don't realize how groundbreaking, it's simple, but it's a groundbreaking idea that bigger data sets will result in better performance.

37
00:03:50,337 --> 00:03:52,639
Speaker 1: It was controversial at the time.

38
00:03:52,899 --> 00:04:05,067
Speaker 1: Some of my well-meaning friends, senior people in the machine learning community, I won't name, but some of whom we know, My well-meaning friends came and were trying to give me friendly advice, like, hey, Andrew, why are you doing this?

39
00:04:05,087 --> 00:04:05,647
Speaker 1: This is crazy.

40
00:04:06,088 --> 00:04:07,368
Speaker 1: It's in the near-natural architecture.

41
00:04:07,388 --> 00:04:08,669
Speaker 1: Look at these architectures we're building.

42
00:04:09,049 --> 00:04:10,089
Speaker 1: You just want to go for scale?

43
00:04:10,109 --> 00:04:11,550
Speaker 1: This is a bad career move.

44
00:04:11,590 --> 00:04:16,332
Speaker 1: So my well-meaning friends, some of them were trying to talk me out of it.

45
00:04:18,233 --> 00:04:26,576
Speaker 1: But I find that if you want to make a breakthrough, you sometimes have to have conviction and do something before it's popular, since that lets you have a bigger impact.

46
00:04:27,273 --> 00:04:29,654
Speaker 0: Let me ask you just a small tangent on that topic.

47
00:04:30,295 --> 00:04:46,543
Speaker 0: I find myself arguing with people saying that greater scale, especially in the context of active learning, so very carefully selecting the data set, but growing the scale of the data set, is going to lead to even further breakthroughs in deep learning.

48
00:04:46,963 --> 00:04:57,746
Speaker 0: And there's currently pushback at that idea, that larger datasets are no longer, so you wanna increase the efficiency of learning, you wanna make better learning mechanisms.

49
00:04:58,226 --> 00:05:05,488
Speaker 0: And I personally believe that just bigger datasets will still, with the same learning methods we have now, will result in better performance.

50
00:05:05,988 --> 00:05:12,070
Speaker 0: What's your intuition at this time on this dual side?

51
00:05:12,090 --> 00:05:20,954
Speaker 0: Do we need to come up with better architectures for learning or can we just get bigger, better data sets that will improve performance?

52
00:05:21,874 --> 00:05:22,935
Speaker 1: I think both are important.

53
00:05:23,435 --> 00:05:24,916
Speaker 1: And it's also problem dependent.

54
00:05:25,136 --> 00:05:33,162
Speaker 1: So for a few data sets, we may be approaching a Bayes error rate or approaching or surpassing human level performance.

55
00:05:33,463 --> 00:05:37,386
Speaker 1: And then there's that theoretical ceiling that we will never surpass a Bayes error rate.

56
00:05:38,166 --> 00:05:48,397
Speaker 1: But then I think there are plenty of problems where we're still quite far from either human level performance or from Bayes array and bigger data sets with neural networks.

57
00:05:49,178 --> 00:05:53,303
Speaker 1: without further algorithm innovation will be sufficient to take us further.

58
00:05:54,664 --> 00:06:04,699
Speaker 1: But on the flip side, if we look at the recent breakthroughs using transformer networks for language models, it was a combination of novel architecture, but also scale had a lot to do with it.

59
00:06:04,739 --> 00:06:09,706
Speaker 1: If we look at what happened with GPT-II and BERT, I think scale was a large part of the story.

60
00:06:10,487 --> 00:06:24,075
Speaker 0: Yeah, that's not often talked about is the scale of the data set it was trained on and the quality of the data set because there's some, so it was like redded threads that had, they were uprated highly.

61
00:06:24,116 --> 00:06:30,580
Speaker 0: So there's already some weak supervision on a very large data set that people don't often talk about, right?

62
00:06:31,460 --> 00:06:38,345
Speaker 1: I find that today we have maturing processes to managing code, things like Git, right, version control.

63
00:06:38,885 --> 00:06:41,547
Speaker 1: It took us a long time to evolve the good processes.

64
00:06:41,807 --> 00:06:49,092
Speaker 1: I remember when my friends and I were emailing each other C++ files in email, you know, but then we had, was it CVS or version Git?

65
00:06:49,432 --> 00:06:50,493
Speaker 1: Maybe something else in the future.

66
00:06:51,754 --> 00:06:58,818
Speaker 1: We're very mature in terms of tools for managing data and thinking about how to clean data and how to solve very hot, messy data problems.

67
00:06:59,679 --> 00:07:00,960
Speaker 1: I think there's a lot of innovation there.

68
00:07:01,500 --> 00:07:02,202
Speaker 1: to be had still.

69
00:07:03,084 --> 00:07:05,309
Speaker 0: I love the idea that you were versioning through email.

70
00:07:06,348 --> 00:07:07,329
Speaker 1: I'll give you one example.

71
00:07:08,089 --> 00:07:20,318
Speaker 1: When we work with manufacturing companies, it's not at all uncommon for there to be multiple labellists that disagree with each other, right?

72
00:07:20,538 --> 00:07:28,523
Speaker 1: And so we would, doing the work in visual inspection, we will take, say, a plastic pot and show it to one inspector.

73
00:07:28,984 --> 00:07:34,828
Speaker 1: And the inspector, sometimes very opinionated, they'll go, clearly that's a defect, a scratch, unacceptable, gotta reject this pot.

74
00:07:35,488 --> 00:07:42,872
Speaker 1: take the same part to a different inspector, different, very affinity, clearly this scratch is small, it's fine, don't throw it away, you're going to make SEOs.

75
00:07:43,572 --> 00:07:55,998
Speaker 1: And then sometimes you take the same plastic part, show it to the same inspector in the afternoon as opposed to in the morning, and very affinity go in the morning to say, clearly this is okay, in the afternoon, equally confident, clearly this is a defect.

76
00:07:56,578 --> 00:08:03,882
Speaker 1: And so what is an AI team supposed to do if sometimes even one person doesn't agree with himself or herself in the span of a day?

77
00:08:04,582 --> 00:08:12,724
Speaker 1: So I think these are the types of very practical, very messy data problems that my teams wrestle with.

78
00:08:14,484 --> 00:08:22,026
Speaker 1: In the case of large consumer internet companies, where you have a billion users, you have a lot of data, you don't worry about it, just take the average, it kind of works.

79
00:08:22,566 --> 00:08:33,554
Speaker 1: But in the case of other industry settings, we don't have big data or just a small data, very small data sets, maybe you have a hundred defective parts or a hundred examples of a defect.

80
00:08:34,054 --> 00:08:42,342
Speaker 1: If you have only a hundred examples, these little labeling errors, if ten of your hundred labels are wrong, that actually is ten percent of your data set has a big impact.

81
00:08:42,842 --> 00:08:43,804
Speaker 1: So how do you clean this up?

82
00:08:43,864 --> 00:08:44,744
Speaker 1: What are you supposed to do?

83
00:08:45,404 --> 00:08:56,049
Speaker 1: This is an example of the types of things that my teams, this is a landing AI example, are wrestling with to deal with small data, which comes up all the time once you're outside consumer internet.

84
00:08:56,389 --> 00:08:57,229
Speaker 0: Yeah, that's fascinating.

85
00:08:57,249 --> 00:09:01,891
Speaker 0: So then you invest more effort and time in thinking about the actual labeling process.

86
00:09:02,311 --> 00:09:03,332
Speaker 0: What are the labels?

87
00:09:03,832 --> 00:09:09,794
Speaker 0: What are the, how are disagreements resolved and all those kinds of like pragmatic real world problems.

88
00:09:09,934 --> 00:09:10,995
Speaker 0: That's a fascinating space.

89
00:09:11,482 --> 00:09:25,687
Speaker 1: Yeah, I find that actually when I'm teaching at Stanford, I increasingly encourage students at Stanford to try to find their own project for the end of term project, rather than just downloading someone else's nicely clean data set.

90
00:09:26,227 --> 00:09:37,971
Speaker 1: It's actually much harder if you need to go and define your own problem and find your own data set, rather than go to one of the several good websites, very good websites with clean scoped data sets that you could just work on.

