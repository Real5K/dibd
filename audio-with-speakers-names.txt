Lex Fridman: You were a proponent of deep learning before it gained widespread acceptance.  What did you see in this field that gave you confidence?  What was your thinking process like in that first decade of the, I don't know what that's called, two thousands, the aughts?  

Andrew Ng: Yeah, I can tell you the thing we got wrong and the thing we got right.  The thing we really got wrong was the importance of, the early importance of unsupervised learning.  So, early days of Google Brain, we put a lot of effort into unsupervised learning rather than supervised learning.  And there was this argument, I think it was around, um, two thousand five, after, you know, NeurIPS, at that time called NIPS, but now NeurIPS had ended, and Jeff Hinton and I were sitting in the cafeteria outside, you know, the conference, we had lunch, we were just chatting.  And Jeff pulled out this napkin, he started sketching this argument on the napkin.  It was very compelling, I'll repeat it.  Human brain has about a hundred trillion, so that's ten to the fourteen synaptic connections.  You will live for about ten to the nine seconds.  That's thirty years.  You actually live for two by ten to the nine, maybe three by ten to the nine seconds.  So just let's say ten to the nine.  So if each synaptic connection, each weight in your brain's neural network has just a one bit parameter, that's ten to the fourteen bits you need to learn in up to ten to the nine seconds of your life.  So via this simple argument, which is a lot of problems, it's very simplified, that's ten to the five bits per second you need to learn in your life.  And I have a one-year-old daughter.  I am not pointing out ten to five bits per second of labels to her.  So, and I think I'm a very loving parent, but I'm just not going to do that, right?  So from this, you know, very crude, definitely problematic argument, there's just no way that most of what we know is through supervised learning.  The way she gets so many bits of information is from sucking in images, audio, those experiences in the world.  And so that argument, and there are a lot of known flaws in this argument, you know, we should go into, really convinced me that there's a lot of power to unsupervised learning.  So that was the part that we actually maybe got wrong.  I still think unsupervised learning is really important, but in the early days, you know, ten, fifteen years ago, a lot of us thought that was the path forward.  

Lex Fridman: Oh, so you're saying that that perhaps was the wrong intuition for the time.  

Andrew Ng: For the time.  That was the part we got wrong.  The part we got right was the importance of scale.  So Adam Coates, another wonderful person, fortunate to have worked with him, he was in my group at Stanford at the time, and Adam had run these experiments at Stanford, showing that the bigger we train a learning algorithm, the better its performance.  And it was based on that, there was a graph that Adam generated, you know, where the x-axis, y-axis lines going up into the right.  So the bigger you make this thing, the better its performance, accuracy is the vertical axis.  So it's really based on that chart that Adam generated, that he gave me the conviction that we could scale these models way bigger than what we could on a few CPUs, which is what we had at Stanford, that we could get even better results.  And it was really based on that one figure that Adam generated.  that gave me the conviction to go with Sebastian Thrun to pitch, you know, starting a project at Google, which became the Google Brain Project.  

Lex Fridman: Google Brain, you go find Google Brain, and there the intuition was scale will bring performance for the system, so we should chase a larger and larger scale.  And I think people don't realize how groundbreaking, it's simple, but it's a groundbreaking idea that bigger data sets will result in better performance.  

Andrew Ng: It was controversial at the time.  Some of my well-meaning friends, senior people in the machine learning community, I won't name, but some of whom we know, My well-meaning friends came and were trying to give me friendly advice, like, hey, Andrew, why are you doing this?  This is crazy.  It's in the near-natural architecture.  Look at these architectures we're building.  You just want to go for scale?  This is a bad career move.  So my well-meaning friends, some of them were trying to talk me out of it.  But I find that if you want to make a breakthrough, you sometimes have to have conviction and do something before it's popular, since that lets you have a bigger impact.  

Lex Fridman: Let me ask you just a small tangent on that topic.  I find myself arguing with people saying that greater scale, especially in the context of active learning, so very carefully selecting the data set, but growing the scale of the data set, is going to lead to even further breakthroughs in deep learning.  And there's currently pushback at that idea, that larger datasets are no longer, so you wanna increase the efficiency of learning, you wanna make better learning mechanisms.  And I personally believe that just bigger datasets will still, with the same learning methods we have now, will result in better performance.  What's your intuition at this time on this dual side?  Do we need to come up with better architectures for learning or can we just get bigger, better data sets that will improve performance?  

Andrew Ng: I think both are important.  And it's also problem dependent.  So for a few data sets, we may be approaching a Bayes error rate or approaching or surpassing human level performance.  And then there's that theoretical ceiling that we will never surpass a Bayes error rate.  But then I think there are plenty of problems where we're still quite far from either human level performance or from Bayes array and bigger data sets with neural networks.  without further algorithm innovation will be sufficient to take us further.  But on the flip side, if we look at the recent breakthroughs using transformer networks for language models, it was a combination of novel architecture, but also scale had a lot to do with it.  If we look at what happened with GPT-II and BERT, I think scale was a large part of the story.  

Lex Fridman: Yeah, that's not often talked about is the scale of the data set it was trained on and the quality of the data set because there's some, so it was like redded threads that had, they were uprated highly.  So there's already some weak supervision on a very large data set that people don't often talk about, right?  

Andrew Ng: I find that today we have maturing processes to managing code, things like Git, right, version control.  It took us a long time to evolve the good processes.  I remember when my friends and I were emailing each other C++ files in email, you know, but then we had, was it CVS or version Git?  Maybe something else in the future.  We're very mature in terms of tools for managing data and thinking about how to clean data and how to solve very hot, messy data problems.  I think there's a lot of innovation there.  to be had still.  

Lex Fridman: I love the idea that you were versioning through email.  

Andrew Ng: I'll give you one example.  When we work with manufacturing companies, it's not at all uncommon for there to be multiple labellists that disagree with each other, right?  And so we would, doing the work in visual inspection, we will take, say, a plastic pot and show it to one inspector.  And the inspector, sometimes very opinionated, they'll go, clearly that's a defect, a scratch, unacceptable, gotta reject this pot.  take the same part to a different inspector, different, very affinity, clearly this scratch is small, it's fine, don't throw it away, you're going to make SEOs.  And then sometimes you take the same plastic part, show it to the same inspector in the afternoon as opposed to in the morning, and very affinity go in the morning to say, clearly this is okay, in the afternoon, equally confident, clearly this is a defect.  And so what is an AI team supposed to do if sometimes even one person doesn't agree with himself or herself in the span of a day?  So I think these are the types of very practical, very messy data problems that my teams wrestle with.  In the case of large consumer internet companies, where you have a billion users, you have a lot of data, you don't worry about it, just take the average, it kind of works.  But in the case of other industry settings, we don't have big data or just a small data, very small data sets, maybe you have a hundred defective parts or a hundred examples of a defect.  If you have only a hundred examples, these little labeling errors, if ten of your hundred labels are wrong, that actually is ten percent of your data set has a big impact.  So how do you clean this up?  What are you supposed to do?  This is an example of the types of things that my teams, this is a landing AI example, are wrestling with to deal with small data, which comes up all the time once you're outside consumer internet.  

Lex Fridman: Yeah, that's fascinating.  So then you invest more effort and time in thinking about the actual labeling process.  What are the labels?  What are the, how are disagreements resolved and all those kinds of like pragmatic real world problems.  That's a fascinating space.  

Andrew Ng: Yeah, I find that actually when I'm teaching at Stanford, I increasingly encourage students at Stanford to try to find their own project for the end of term project, rather than just downloading someone else's nicely clean data set.  It's actually much harder if you need to go and define your own problem and find your own data set, rather than go to one of the several good websites, very good websites with clean scoped data sets that you could just work on.  